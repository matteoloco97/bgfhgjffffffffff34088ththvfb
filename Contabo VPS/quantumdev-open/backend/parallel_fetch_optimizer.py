#!/usr/bin/env python3
"""
parallel_fetch_optimizer.py
============================
Ottimizzazione CRITICA per ridurre latency da 24-30s a 3-5s.

PROBLEMA ATTUALE:
- fetch_and_extract() Ã¨ SINCRONA e chiamata in loop
- Blocking I/O su ogni URL sequenzialmente
- Timeout aggregati: 3.5s Ã— 5 URLs = 17.5s + overhead

SOLUZIONE:
- Fetch parallelo con asyncio.gather()
- Timeout per singolo URL, non somma
- Early success: se 2/5 OK â†’ termina subito

GUADAGNO ATTESO:
- Latenza: da 24s a 3-5s (-80%)
- Success rate: +10% (meno timeout totali)
"""

import asyncio
import time
from typing import List, Dict, Any, Tuple, Optional
import logging

log = logging.getLogger(__name__)


# ==================== VERSIONE OTTIMIZZATA ====================

async def parallel_fetch_and_extract(
    results: List[Dict[str, Any]],
    max_concurrent: int = 4,
    timeout_per_url: float = 4.0,
    min_successful: int = 2,
    prioritize_quality: bool = True,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Fetch parallelo di URLs con early exit e stats dettagliate.
    OPTIMIZED: Aggiunta prioritizzazione qualitÃ  e backpressure control.
    
    Args:
        results: Lista di dict con almeno 'url', 'title'
        max_concurrent: Max fetch simultanei (default 4)
        timeout_per_url: Timeout PER SINGOLO URL (default 4s)
        min_successful: Esci appena hai N successi (default 2)
        prioritize_quality: Se True, ordina risultati per qualitÃ  stimata
    
    Returns:
        (extracted_docs, stats)
        extracted_docs: Lista di dict con {url, title, text, og_image}
        stats: Dict con metriche (attempted, ok, timeouts, errors, duration_ms)
    """
    
    # OPTIMIZATION: Early return per input vuoto
    if not results:
        return [], {"attempted": 0, "ok": 0, "timeouts": 0, "errors": 0, "duration_ms": 0, "early_exit": False}
    
    # Import lazy per non rompere se modulo manca
    try:
        from core.web_tools import fetch_and_extract_robust
    except ImportError:
        log.error("âŒ fetch_and_extract_robust not found, fallback to sync version")
        try:
            from core.web_tools import fetch_and_extract
            
            # Wrap sincrona in async
            async def fetch_and_extract_robust(url: str, timeout: float = 4.0):
                loop = asyncio.get_running_loop()
                return await loop.run_in_executor(None, fetch_and_extract, url, timeout)
        except ImportError:
            log.error("âŒ No fetch function available!")
            return [], {"error": "fetch_unavailable"}
    
    t_start = time.perf_counter()
    
    extracted: List[Dict[str, Any]] = []
    timeouts = 0
    errors = 0
    attempted = 0
    
    # OPTIMIZATION: Aumenta concorrenza se molti URL
    effective_concurrent = min(max_concurrent, len(results), 6)  # Max 6 paralleli
    
    # Semaforo per limitare concorrenza
    semaphore = asyncio.Semaphore(effective_concurrent)
    
    async def _fetch_one(item: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:
        """Fetch singolo URL con timeout e error handling"""
        nonlocal timeouts, errors, attempted
        
        url = item.get("url", "")
        if not url:
            return None
        
        attempted += 1
        
        async with semaphore:
            try:
                # Timeout per QUESTO URL specifico
                text, og_img = await asyncio.wait_for(
                    fetch_and_extract_robust(url, timeout=timeout_per_url),
                    timeout=timeout_per_url + 1.0  # +1s safety margin
                )
                
                if not text or len(text) < 100:
                    errors += 1
                    return None
                
                return {
                    "url": url,
                    "title": item.get("title", url),
                    "text": text,
                    "og_image": og_img,
                    "index": idx,
                }
                
            except asyncio.TimeoutError:
                timeouts += 1
                log.warning(f"â±ï¸ Timeout {url}")
                return None
            except Exception as e:
                errors += 1
                log.warning(f"âŒ Error fetching {url}: {e}")
                return None
    
    # Crea task per tutti gli URL
    tasks = [
        asyncio.create_task(_fetch_one(item, idx))
        for idx, item in enumerate(results)
    ]
    
    # Strategy: return_when=FIRST_COMPLETED per early exit
    if min_successful > 0:
        # Aspetta task uno per uno
        pending = set(tasks)
        
        while pending and len(extracted) < min_successful:
            done, pending = await asyncio.wait(
                pending,
                return_when=asyncio.FIRST_COMPLETED,
                timeout=timeout_per_url
            )
            
            for task in done:
                try:
                    result = await task
                    if result:
                        extracted.append(result)
                        log.info(f"âœ… Fetched {len(extracted)}/{min_successful}: {result['url']}")
                        
                        # Early exit!
                        if len(extracted) >= min_successful:
                            log.info(f"ğŸ¯ Early exit: {len(extracted)} docs ready")
                            # Cancella task pendenti
                            for t in pending:
                                t.cancel()
                            break
                except Exception as e:
                    log.warning(f"Task error: {e}")
        
        # Aspetta eventuali task rimasti (se early exit non triggered)
        if pending:
            for task in pending:
                if not task.done():
                    task.cancel()
    
    else:
        # Nessun early exit, aspetta tutti
        results_list = await asyncio.gather(*tasks, return_exceptions=True)
        extracted = [r for r in results_list if isinstance(r, dict) and r.get("text")]
    
    duration_ms = int((time.perf_counter() - t_start) * 1000)
    
    stats = {
        "attempted": attempted,
        "ok": len(extracted),
        "timeouts": timeouts,
        "errors": errors,
        "duration_ms": duration_ms,
        "early_exit": len(extracted) >= min_successful and len(tasks) > len(extracted),
    }
    
    log.info(f"ğŸ“Š Fetch stats: {stats}")
    
    return extracted, stats


# ==================== PATCH PER quantum_api.py ====================

def generate_quantum_api_patch():
    """
    Genera il codice da inserire in quantum_api.py
    per sostituire il loop sincrono con fetch parallelo.
    """
    
    patch = '''
# ============ TROVA QUESTO BLOCCO (circa linea 600-650) ============
#
# Codice VECCHIO da sostituire:
#
# extracts: List[Dict[str, Any]] = []
# attempted, ok_count, timeouts, errors = 0, 0, 0, 0
# t_fetch = time.perf_counter()
# 
# for r in topk[:nsum]:
#     url = r.get("url", "")
#     title = r.get("title", url)
#     attempted += 1
#     try:
#         text, og_img = await asyncio.wait_for(
#             fetch_and_extract(url),
#             timeout=WEB_FETCH_TIMEOUT_S,
#         )
#         if text:
#             extracts.append({"url": url, "title": title, "text": text, "og_image": og_img})
#             ok_count += 1
#     except asyncio.TimeoutError:
#         timeouts += 1
#     except Exception:
#         errors += 1
#
# ============ SOSTITUISCI CON QUESTO ============

# Import all'inizio del file
from parallel_fetch_optimizer import parallel_fetch_and_extract

# Nel corpo di _web_search_pipeline, SOSTITUISCI il loop con:

# Fetch parallelo (NUOVO)
extracts, fetch_stats = await parallel_fetch_and_extract(
    results=topk[:nsum],
    max_concurrent=WEB_FETCH_MAX_INFLIGHT,  # giÃ  definito: default 4
    timeout_per_url=WEB_FETCH_TIMEOUT_S,     # giÃ  definito: 3.5s
    min_successful=2,  # Early exit appena 2 docs pronti
)

# Estrai stats
attempted = fetch_stats.get("attempted", 0)
ok_count = fetch_stats.get("ok", 0)
timeouts = fetch_stats.get("timeouts", 0)
errors = fetch_stats.get("errors", 0)
fetch_duration_ms = fetch_stats.get("duration_ms", 0)
done_early = fetch_stats.get("early_exit", False)

# Il resto del codice rimane identico (synthesis, etc.)
'''
    
    return patch


# ==================== TEST & BENCHMARK ====================

async def benchmark_comparison():
    """
    Confronto diretto: fetch sequenziale vs parallelo
    """
    
    test_urls = [
        {"url": "https://www.ilmeteo.it/", "title": "IlMeteo"},
        {"url": "https://www.3bmeteo.com/", "title": "3BMeteo"},
        {"url": "https://www.meteoam.it/", "title": "MeteoAM"},
        {"url": "https://www.python.org/", "title": "Python.org"},
        {"url": "https://docs.python.org/3/tutorial/", "title": "Python Tutorial"},
    ]
    
    print("\n" + "="*80)
    print("BENCHMARK: Sequential vs Parallel Fetch")
    print("="*80 + "\n")
    
    # Test parallelo
    t_start = time.perf_counter()
    docs, stats = await parallel_fetch_and_extract(
        test_urls,
        max_concurrent=4,
        timeout_per_url=4.0,
        min_successful=2
    )
    parallel_time = time.perf_counter() - t_start
    
    print(f"âœ… PARALLEL FETCH:")
    print(f"   Duration: {parallel_time:.2f}s")
    print(f"   Docs extracted: {len(docs)}")
    print(f"   Stats: {stats}")
    print()
    
    # Simulazione sequenziale (per confronto teorico)
    seq_time_estimate = len(test_urls) * 4.0  # 4s per URL
    
    print(f"âŒ SEQUENTIAL (estimated):")
    print(f"   Duration: ~{seq_time_estimate:.2f}s")
    print(f"   Speedup: {seq_time_estimate / parallel_time:.1f}x")
    print()
    
    print("="*80)
    print(f"CONCLUSION: Parallel is {seq_time_estimate / parallel_time:.1f}x faster!")
    print("="*80 + "\n")


if __name__ == "__main__":
    print(generate_quantum_api_patch())
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                 PARALLEL FETCH DEPLOYMENT GUIDE                       â•‘
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—

STEP 1: Copy file to quantumdev-open
---------------------------------------
scp parallel_fetch_optimizer.py root@your-server:/root/quantumdev-open/

STEP 2: Backup quantum_api.py
-------------------------------
cd /root/quantumdev-open
cp backend/quantum_api.py backend/quantum_api.py.backup-parallel

STEP 3: Edit quantum_api.py
-----------------------------
1. Aggiungi import all'inizio (dopo gli altri from core.*):
   
   from parallel_fetch_optimizer import parallel_fetch_and_extract

2. Trova il blocco con il loop di fetch (circa linea 600-650):
   
   Cerca questo pattern:
   ```
   for r in topk[:nsum]:
       url = r.get("url", "")
       ...
       text, og_img = await asyncio.wait_for(
           fetch_and_extract(url),
   ```

3. SOSTITUISCI l'intero blocco con:
   
   ```python
   # Fetch parallelo (NUOVO)
   extracts, fetch_stats = await parallel_fetch_and_extract(
       results=topk[:nsum],
       max_concurrent=WEB_FETCH_MAX_INFLIGHT,
       timeout_per_url=WEB_FETCH_TIMEOUT_S,
       min_successful=2,
   )
   
   # Estrai stats
   attempted = fetch_stats.get("attempted", 0)
   ok_count = fetch_stats.get("ok", 0)
   timeouts = fetch_stats.get("timeouts", 0)
   errors = fetch_stats.get("errors", 0)
   fetch_duration_ms = fetch_stats.get("duration_ms", 0)
   done_early = fetch_stats.get("early_exit", False)
   ```

STEP 4: Restart service
------------------------
sudo systemctl restart quantum-api

# Verifica logs
sudo journalctl -u quantum-api -f

STEP 5: Test immediato
-----------------------
time curl -X POST "http://127.0.0.1:8081/web/search" \\
  -H "Content-Type: application/json" \\
  -d '{"q": "meteo Roma domani", "k": 5, "source": "test", "source_id": "test"}'

time curl -X POST "http://127.0.0.1:8081/web/search" \\
  -H "Content-Type: application/json" \\
  -d '{"q": "python tutorial", "k": 5, "source": "test", "source_id": "test"}'

RISULTATI ATTESI:
------------------
âœ… "meteo Roma domani": 3-5s (era 24s) â†’ -80% latency
âœ… "python tutorial": 4-6s (era timeout 30s+) â†’ SUCCESS
âœ… Early exit: se 2/5 docs OK â†’ termina subito
âœ… Stats nei logs: "ğŸ“Š Fetch stats: {attempted: 5, ok: 3, timeouts: 1, ...}"

ROLLBACK (se problemi):
------------------------
cd /root/quantumdev-open
mv backend/quantum_api.py backend/quantum_api.py.broken
mv backend/quantum_api.py.backup-parallel backend/quantum_api.py
sudo systemctl restart quantum-api

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # Opzionale: run benchmark se asyncio disponibile
    try:
        asyncio.run(benchmark_comparison())
    except Exception as e:
        print(f"Benchmark skipped: {e}")
